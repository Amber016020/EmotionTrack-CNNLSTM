import os
import tensorflow as tf
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.applications import Xception
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# --- åƒæ•¸ ---
img_size = (96, 96)
batch_size = 32
epochs = 30
fer_path = r'D:\emotionRecognition\Face\EmotionTrack-CNNLSTM\data\train\FER-2013\train'
num_classes = len(os.listdir(fer_path))

# --- è³‡æ–™å¢å¼·è¨­å®š ---
datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    validation_split=0.1
)

train_gen = datagen.flow_from_directory(
    fer_path, target_size=img_size, batch_size=batch_size,
    class_mode='categorical', subset='training'
)

val_gen = datagen.flow_from_directory(
    fer_path, target_size=img_size, batch_size=batch_size,
    class_mode='categorical', subset='validation'
)

# --- æ¨¡å‹å»ºç«‹èˆ‡è¼‰å…¥é è¨“ç·´æ¬Šé‡ ---
base_model_new = Xception(weights='imagenet', include_top=False, input_shape=(96, 96, 3))
x = GlobalAveragePooling2D()(base_model_new.output)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)  # â† Dropout åŠ åœ¨ dense å¾Œï¼Œé˜²æ­¢éæ“¬åˆ
output = Dense(num_classes, activation='softmax')(x)
model_new = Model(inputs=base_model_new.input, outputs=output)

# --- è¼‰å…¥å·ç©å±¤çš„é è¨“ç·´æ¬Šé‡ ---
pretrained_model = load_model('xception_affectnet_pretrained.h5')
for i in range(len(base_model_new.layers)):
    try:
        model_new.layers[i].set_weights(pretrained_model.layers[i].get_weights())
    except:
        print(f"âš ï¸ Skip incompatible layer {i}")

# --- ç·¨è­¯èˆ‡è¨“ç·´ ---
model_new.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

print("ğŸ“Œ Start fine-tuning on FER2013 with data augmentation and dropout...")
history = model_new.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=[early_stop])

# --- æ¨¡å‹å„²å­˜ ---
model_new.save('xception_finetuned_fer2013_dropout_aug.h5')

# --- è¨“ç·´æ›²ç·šåœ– ---
plt.figure(figsize=(10, 4))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', color='royalblue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orangered')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', color='royalblue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orangered')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig("lstm_training_plot_dropout_aug.png", dpi=300)
plt.show()
