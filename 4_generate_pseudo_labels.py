import os
import numpy as np
import json
import cv2
import tempfile
from keras.models import load_model, Model
from tensorflow.keras.preprocessing.image import img_to_array
from tqdm import tqdm
import torch
import timm
from torchvision import transforms
from PIL import Image
import torch.nn as nn

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# è¼‰å…¥ PyTorch æ¨¡å‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = timm.create_model('legacy_xception', pretrained=False, num_classes=8, in_chans=3)
model.load_state_dict(torch.load("xception_model_best.pth", map_location=device))
model.eval().to(device)

# å–ä¸­é–“å±¤ï¼ˆä¾‹å¦‚ avgpool æˆ– flatten å‰çš„è¼¸å‡ºï¼‰ä½œç‚ºç‰¹å¾µ
# æ›¿ä»£ Keras çš„ feature_model
class FeatureExtractor(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.features = nn.Sequential(*list(model.children())[:-1])  # å–å‡ºå…¨é€£æ¥å±¤å‰çš„éƒ¨åˆ†
    def forward(self, x):
        with torch.no_grad():
            return self.features(x).squeeze()

feature_extractor = FeatureExtractor(model).to(device)

# ==== å®‰å…¨å„²å­˜å‡½å¼ï¼ˆ.npy / .jsonï¼‰ ====
def safe_save_npy(path, data):
    dir_path = os.path.dirname(path)
    tmp_fd, tmp_path = tempfile.mkstemp(dir=dir_path)
    with os.fdopen(tmp_fd, 'wb') as tmp_file:
        np.save(tmp_file, data)
    os.replace(tmp_path, path)

def safe_save_json(path, data):
    dir_path = os.path.dirname(path)
    tmp_fd, tmp_path = tempfile.mkstemp(dir=dir_path)
    with os.fdopen(tmp_fd, 'w', encoding='utf-8') as tmp_file:
        json.dump(data, tmp_file, ensure_ascii=False, indent=2)
    os.replace(tmp_path, path)

# ==== è¨­å®šåƒæ•¸ ====
FRAME_DIR = "data/frame/train"
FEATURE_DIR = "data/feature"
CNN_MODEL_PATH = "xception_affectnet_pretrained.h5"
SEQUENCE_SAVE_PATH = os.path.join(FEATURE_DIR, "sequences.npy")
LABEL_PATH = os.path.join(FEATURE_DIR, "pseudo_labels.json")

EMOTION_MAP = {
    "01": 0,  # neutral
    "02": 1,  # calm
    "03": 2,  # happy
    "04": 3,  # sad
    "05": 4,  # angry
    "06": 5,  # fear
    "07": 6,  # disgust
    "08": 7,  # surprise
}

os.makedirs(FEATURE_DIR, exist_ok=True)

# ==== è¼‰å…¥æ¨¡å‹ï¼ˆflatten å±¤ï¼‰ ====
cnn_model = load_model(CNN_MODEL_PATH, compile=False)
feature_model = Model(inputs=cnn_model.input, outputs=cnn_model.get_layer("global_average_pooling2d").output)

# ==== è¼‰å…¥å·²å­˜åœ¨è³‡æ–™ï¼ˆçºŒè·‘ç”¨ï¼‰====
features_dict = {}
label_dict = {}

if os.path.exists(SEQUENCE_SAVE_PATH):
    features_dict = np.load(SEQUENCE_SAVE_PATH, allow_pickle=True).item()
    print(f"ğŸ” Loaded existing features: {len(features_dict)}")

if os.path.exists(LABEL_PATH):
    with open(LABEL_PATH, "r", encoding="utf-8") as f:
        label_dict = json.load(f)
    print(f"ğŸ” Loaded existing labels: {len(label_dict)}")

# ==== ç‰¹å¾µæŠ½å–å‡½å¼ ====
transform = transforms.Compose([
    transforms.Resize((299, 299)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

def extract_feature_vector(image_path):
    img = Image.open(image_path).convert("RGB")
    img = transform(img).unsqueeze(0).to(device)
    features = feature_extractor(img)  # shape: (2048,)
    return features.cpu().numpy()

# ==== ä¸»æµç¨‹ï¼šæ¯éƒ¨å½±ç‰‡è™•ç† ====
processed_since_last_save = 0

for video_id in tqdm(os.listdir(FRAME_DIR)):
    if video_id in features_dict:
        continue  # å·²è™•ç†é

    video_path = os.path.join(FRAME_DIR, video_id)
    if not os.path.isdir(video_path):
        continue

    print(f"ğŸ“ Processing: {video_id}")

    # æ“·å– label
    parts = video_id.split("-")
    if len(parts) >= 3:
        emotion_code = parts[2]
        if emotion_code in EMOTION_MAP:
            label_dict[video_id] = EMOTION_MAP[emotion_code]
        else:
            print(f"âš ï¸ Unknown emotion code in: {video_id}")
            continue
    else:
        print(f"âš ï¸ Unexpected video ID format: {video_id}")
        continue

    # æŠ½ç‰¹å¾µåºåˆ—
    image_files = sorted(os.listdir(video_path), key=lambda x: int(x.split("_")[1].split(".")[0]))
    sequence = []
    for img_name in image_files:
        img_path = os.path.join(video_path, img_name)
        try:
            feature_vector = extract_feature_vector(img_path)
            sequence.append(feature_vector)
        except Exception as e:
            print(f"âš ï¸ Error processing {img_path} â†’ {e}")

    if sequence:
        features_dict[video_id] = np.array(sequence)
        processed_since_last_save += 1

        # æ¯è™•ç†10éƒ¨å½±ç‰‡å°±å„²å­˜ä¸€æ¬¡
        if processed_since_last_save >= 10:
            safe_save_npy(SEQUENCE_SAVE_PATH, features_dict)
            safe_save_json(LABEL_PATH, label_dict)
            print(f"ğŸ’¾ Auto-saved after processing 10 videos.")
            processed_since_last_save = 0

# æœ€å¾Œä¸€æ¬¡å„²å­˜ï¼ˆè™•ç†æ•¸ä¸æ˜¯10çš„å€æ•¸æ™‚ï¼‰
if processed_since_last_save > 0:
    safe_save_npy(SEQUENCE_SAVE_PATH, features_dict)
    safe_save_json(LABEL_PATH, label_dict)
    print(f"ğŸ’¾ Final auto-save of remaining videos.")

# ==== å®Œæˆè¨Šæ¯ ====
print("âœ… Feature extraction completed.")
print(f"ğŸ“¦ Total videos processed: {len(features_dict)}")
print(f"âœ… Labels saved to {LABEL_PATH}")