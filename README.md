# ğŸ“„ README.md

# EmotionTrack-CNNLSTM

A deep learning-based facial emotion recognition system designed to analyze emotional transitions in video sequences. This project integrates **CNN** and **LSTM** architectures to capture both spatial and temporal patterns in facial expressions, enabling fine-grained emotion trend analysis over time.

---

## ğŸ§  Features
- **CNN-based Emotion Recognition:** Trained on the Kaggle FER2013 dataset to classify 7 basic emotions.
- **Sequential Modeling with LSTM:** Captures temporal dependencies across video frames to improve emotion prediction accuracy.
- **Automated Video Processing:**
  - Extracts frames from user-recorded videos.
  - Organizes frames into directories based on user ID and experimental condition.
- **Emotion Labeling:**
  - Uses pre-trained CNN model to label each frame's emotion.
  - Saves labeled results in a structured `.npy` format.
- **Emotion Trend Visualization:**
  - Smooths emotion predictions over time.
  - Annotates peaks & valleys with corresponding facial frames.
  - Generates clear visual charts per user & condition.
- **Model Evaluation:** Calculates accuracy, F1-score, confusion matrix to assess model performance.
  
---

## ğŸ—‚ï¸ Folder Structure

```
EmotionTrack-CNNLSTM
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ videos               # Raw video files (one per minute)
â”‚   â”œâ”€â”€ frames               # Extracted frames organized by user_id/condition
â”‚   â””â”€â”€ labeled_frames       # Emotion labels saved in .npy
â”œâ”€â”€ models
â”‚   â””â”€â”€ facialemotionmodel.h5  # Pre-trained CNN model
â”‚   â””â”€â”€ lstm_model.h5          # Trained CNN+LSTM model
â”œâ”€â”€ results
â”‚   â””â”€â”€ emotion_trend_X_X.png  # Visualized emotion trend plots
â”œâ”€â”€ 1_extract_frames.py      # Extract frames & organize folders
â”œâ”€â”€ 2_label_frames.py        # CNN model labels emotion on frames
â”œâ”€â”€ 3_train_cnn_lstm.py      # Train CNN + LSTM sequence model
â”œâ”€â”€ 4_evaluate_model.py      # Evaluate model accuracy & F1-score
â”œâ”€â”€ 5_visualize_emotion.py   # Plot emotion transitions + annotate frames
â””â”€â”€ README.md
```

---

## ğŸš€ Workflow Overview

1. **Frame Extraction:**
   - `1_extract_frames.py`  
   - Extracts 5 FPS frames from videos, categorizes them into `user_id/condition` folders.

2. **Emotion Labeling:**
   - `2_label_frames.py`  
   - Uses pre-trained CNN to label each frame's emotion â†’ saves structured `.npy`.

3. **Model Training:**
   - `3_train_cnn_lstm.py`  
   - Constructs sequences of frames (length = 5), feeds into CNN+LSTM for training.
   - Saves trained model `lstm_model.h5`.

4. **Model Evaluation:**
   - `4_evaluate_model.py`  
   - Calculates accuracy, F1-score, confusion matrix.

5. **Emotion Trend Visualization:**
   - `5_visualize_emotion.py`  
   - Smooths predictions over time.
   - Annotates peaks & valleys with actual frame images.
   - Saves charts for each user-condition.

---

## ğŸ“Š Emotion Categories

| Index | Emotion  |
|------|---------|
| 0    | Angry   |
| 1    | Disgust |
| 2    | Fear    |
| 3    | Happy   |
| 4    | Neutral |
| 5    | Sad     |
| 6    | Surprise|

---

## ğŸ“¦ Dependencies

- Python 3.x
- Keras
- TensorFlow
- OpenCV
- NumPy
- scikit-learn
- matplotlib
- tqdm
- scipy

Install:

```bash
pip install -r requirements.txt
```

---

## ğŸ’¡ Possible Applications
- Affective computing in human-computer interaction (HCI).
- User behavior analysis during real-time communication.
- Psychological studies on emotion dynamics.

---

## ğŸ“‘ Future Work Ideas
- Integrate real-time streaming emotion feedback.
- Fine-tune on in-domain datasets (real user video data).
- Expand to multi-modal analysis (text, audio + facial).
